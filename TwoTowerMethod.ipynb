{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config",
   "id": "4882c408d2f9c97d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-16T21:11:04.750501Z",
     "start_time": "2026-02-16T21:11:04.720723Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    interactions_path: str = \"data/cb_baseline.parquet\"\n",
    "    items_path: str = \"data/geo_feature_matrix.csv\"\n",
    "    artifacts_dir: str = \"artifacts\"\n",
    "\n",
    "    embed_dim: int = 128\n",
    "    hidden_dim: int =512\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    epochs: int = 30 #10\n",
    "    batch_size: int = 2048 #2048\n",
    "\n",
    "    # label rule\n",
    "    # centered_rating > 0 => positive\n",
    "    pos_threshold: float = 0.0\n",
    "\n",
    "    # evaluation    \n",
    "    k: int = 10\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model definition\n",
    "Definition for TwoTower Model. Input item tower and user tower and output a score that user love this item or not."
   ],
   "id": "17b05211c57a0c00"
  },
  {
   "cell_type": "code",
   "id": "e3cdb7d43e2d887f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:11:04.769816Z",
     "start_time": "2026-02-16T21:11:04.759862Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Model definition for TwoTower Model. Input item tower and user tower and output a score that user love this item or not.\n",
    "class TwoTower(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128, embed_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_x, item_x):\n",
    "        u = self.user_tower(user_x)\n",
    "        i = self.item_tower(item_x)\n",
    "        # dot product\n",
    "        return (u * i).sum(dim=1)"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data load/clean/fitter\n",
   "id": "ec9a19b214dd76e5"
  },
  {
   "cell_type": "code",
   "id": "d0a6354c2ade3207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:11:04.785284Z",
     "start_time": "2026-02-16T21:11:04.770597Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 读文件/清理列名/统一数据类型\n",
    "def load_raw(interactions_path: str, items_path: str):\n",
    "    interactions = pd.read_parquet(interactions_path)\n",
    "    items = pd.read_csv(items_path)\n",
    "    def load_raw(interactions_path: str, items_path: str):\n",
    "        interactions = pd.read_parquet(interactions_path)\n",
    "        items = pd.read_csv(items_path)\n",
    "\n",
    "        # 清理列名空格/不可见字符，避免 merge / 筛列失败\n",
    "        interactions.columns = interactions.columns.str.strip()\n",
    "        items.columns = items.columns.str.strip()\n",
    "\n",
    "        # items到底有多少列\n",
    "        # print(\"[DEBUG] items shape:\", items.shape)\n",
    "        # print(\"[DEBUG] first 30 item columns:\", list(items.columns[:30]))\n",
    "        # print(\"[DEBUG] last  30 item columns:\", list(items.columns[-30:]))\n",
    "\n",
    "        interactions[\"business_id\"] = interactions[\"business_id\"].astype(str)\n",
    "        items[\"business_id\"] = items[\"business_id\"].astype(str)\n",
    "        interactions[\"user_id\"] = interactions[\"user_id\"].astype(str)\n",
    "\n",
    "        return interactions, items\n",
    "\n",
    "\n",
    "    # unify types\n",
    "    interactions[\"business_id\"] = interactions[\"business_id\"].astype(str)\n",
    "    items[\"business_id\"] = items[\"business_id\"].astype(str)\n",
    "    interactions[\"user_id\"] = interactions[\"user_id\"].astype(str)\n",
    "\n",
    "    return interactions, items\n",
    "\n",
    "# 自动特征选择\n",
    "def get_feature_cols(items: pd.DataFrame):\n",
    "    # 先排除 id\n",
    "    candidate = items.drop(columns=[\"business_id\"], errors=\"ignore\")\n",
    "\n",
    "    # 只保留“看起来像数值”的列：要么本来就是数值 dtype，\n",
    "    # 要么能被安全转换成数值（比如object但内容是 \"25.0\"）\n",
    "    numeric_cols = []\n",
    "    for c in candidate.columns:\n",
    "        s = pd.to_numeric(candidate[c], errors=\"coerce\")\n",
    "        # 至少有一部分不是 NaN 才算有效特征\n",
    "        if s.notna().mean() > 0.0001:\n",
    "            numeric_cols.append(c)\n",
    "\n",
    "    # print(\"[DEBUG] feature cols:\", len(numeric_cols))\n",
    "    return numeric_cols\n",
    "\n",
    "# 合并，把 item 特征附加到交互记录上。\n",
    "def make_merged_df(interactions: pd.DataFrame, items: pd.DataFrame):\n",
    "    df = interactions.merge(items, on=\"business_id\", how=\"inner\")\n",
    "    return df\n",
    "\n",
    "# 构造标签（大于pos_threshold 就是正标签，反之亦然）\n",
    "def make_label(df: pd.DataFrame, pos_threshold: float = 0.0):\n",
    "    if \"centered_rating\" not in df.columns:\n",
    "        raise ValueError(\"cb_baseline.parquet must contain centered_rating column.\")\n",
    "    df[\"label\"] = (df[\"centered_rating\"] > pos_threshold).astype(int)\n",
    "    return df\n",
    "\n",
    "# 特征处理（保证非负，log压缩长尾）\n",
    "def numeric_log1p_clip(df: pd.DataFrame, cols):\n",
    "    x = df[cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    x = x.clip(lower=0.0)\n",
    "    return np.log1p(x.to_numpy(dtype=np.float32))\n",
    "\n",
    "# 用户画像（特征向量）（对所有正相关数据取平均以构成用户画像）\n",
    "def build_user_profiles(df: pd.DataFrame, feature_cols):\n",
    "    # user profile = mean of positive items' content features\n",
    "    pos = df[df[\"label\"] == 1].copy()\n",
    "    if len(pos) == 0:\n",
    "        raise ValueError(\"No positive samples found. Check pos_threshold or centered_rating distribution.\")\n",
    "    for c in feature_cols:\n",
    "        pos[c] = pd.to_numeric(pos[c], errors=\"coerce\")\n",
    "    pos[feature_cols] = pos[feature_cols].fillna(0.0)\n",
    "    profiles = pos.groupby(\"user_id\")[feature_cols].mean()\n",
    "    return profiles\n",
    "# 构造训练数据\n",
    "def build_training_matrix(df: pd.DataFrame, user_profiles: pd.DataFrame, feature_cols):\n",
    "    # attach user_profile columns to each interaction row\n",
    "    train_df = df.merge(user_profiles, on=\"user_id\", how=\"inner\", suffixes=(\"\", \"_user\"))\n",
    "    if len(train_df) == 0:\n",
    "        raise ValueError(\"train_df is empty after merging user_profiles. Possibly no users with positives in df.\")\n",
    "    user_feature_cols = [f\"{c}_user\" for c in feature_cols]\n",
    "    return train_df, user_feature_cols\n",
    "# 标准化（训练）\n",
    "def fit_scaler_and_transform(train_df: pd.DataFrame, feature_cols, user_feature_cols):\n",
    "    X_item = numeric_log1p_clip(train_df, feature_cols)\n",
    "    X_user = numeric_log1p_clip(train_df, user_feature_cols)\n",
    "    y = train_df[\"label\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_item = scaler.fit_transform(X_item).astype(np.float32)\n",
    "    X_user = scaler.transform(X_user).astype(np.float32)\n",
    "    return X_user, X_item, y, scaler\n",
    "# 预测时数据处理\n",
    "def transform_all_items(items: pd.DataFrame, feature_cols, scaler: StandardScaler):\n",
    "    # IMPORTANT: recommend over UNIQUE restaurants, not interaction rows\n",
    "    tmp = items[[\"business_id\"] + feature_cols].copy()\n",
    "    X_item = numeric_log1p_clip(tmp, feature_cols)\n",
    "    X_item = scaler.transform(X_item).astype(np.float32)\n",
    "    item_ids = tmp[\"business_id\"].to_numpy()\n",
    "    return item_ids, X_item\n",
    "\n",
    "def save_metadata(artifacts_dir: str, feature_cols):\n",
    "    os.makedirs(artifacts_dir, exist_ok=True)\n",
    "    with open(os.path.join(artifacts_dir, \"feature_cols.json\"), \"w\") as f:\n",
    "        json.dump(feature_cols, f)"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pytorch Debug",
   "id": "1a5060aeb8d9e6b1"
  },
  {
   "cell_type": "code",
   "id": "5f57a189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:11:04.789884Z",
     "start_time": "2026-02-16T21:11:04.785921Z"
    }
   },
   "source": [
    "# import torch, sys\n",
    "# print(\"python:\", sys.version)\n",
    "# print(\"torch:\", torch.__version__)\n",
    "# print(\"torch cuda:\", torch.version.cuda)\n",
    "# print(\"cuda available:\", torch.cuda.is_available())\n",
    "# print(\"device count:\", torch.cuda.device_count())\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "#     print(\"archs:\", torch.cuda.get_arch_list())\n"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "id": "71ceaf2c0012d244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:16:03.635683Z",
     "start_time": "2026-02-16T21:11:04.790382Z"
    }
   },
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import random\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# 训练用 Bayesian Personalized Ranking（更注重排序）\n",
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, df_pos, user_cache, feature_cols, items_df, scaler):\n",
    "        self.df_pos = df_pos[[\"user_id\",\"business_id\"]].reset_index(drop=True)\n",
    "        self.user_cache = user_cache\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "        # 预处理好的“全量item特征矩阵”\n",
    "        self.item_ids, self.item_X = transform_all_items(items_df, feature_cols, scaler)\n",
    "        self.id2idx = {bid:i for i,bid in enumerate(self.item_ids)}\n",
    "        self.all_item_ids = list(self.id2idx.keys())\n",
    "\n",
    "        self.user_pos = (\n",
    "            df_pos.groupby(\"user_id\")[\"business_id\"].apply(set).to_dict()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid = self.df_pos.loc[idx, \"user_id\"]\n",
    "        pos_bid = self.df_pos.loc[idx, \"business_id\"]\n",
    "\n",
    "        # user vector：直接缓存取\n",
    "        u_x = self.user_cache[uid].astype(np.float32)\n",
    "\n",
    "        # pos item\n",
    "        pos_i = self.item_X[self.id2idx[pos_bid]]\n",
    "\n",
    "        # sample neg item\n",
    "        while True:\n",
    "            neg_bid = random.choice(self.all_item_ids)\n",
    "            if neg_bid not in self.user_pos.get(uid, set()):\n",
    "                break\n",
    "        neg_i = self.item_X[self.id2idx[neg_bid]]\n",
    "\n",
    "        return torch.from_numpy(u_x), torch.from_numpy(pos_i), torch.from_numpy(neg_i)\n",
    "\n",
    "# AUC测试\n",
    "class RecDataset(Dataset):\n",
    "    def __init__(self, X_user, X_item, y):\n",
    "        self.X_user = torch.from_numpy(X_user)\n",
    "        self.X_item = torch.from_numpy(X_item)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_user[idx], self.X_item[idx], self.y[idx]\n",
    "\n",
    "def bpr_loss(pos_score, neg_score):\n",
    "    return -torch.log(torch.sigmoid(pos_score - neg_score) + 1e-8).mean()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    cfg = Config()\n",
    "    os.makedirs(cfg.artifacts_dir, exist_ok=True)\n",
    "\n",
    "    interactions, items = load_raw(cfg.interactions_path, cfg.items_path)\n",
    "    feature_cols = get_feature_cols(items)\n",
    "\n",
    "    df = make_merged_df(interactions, items)\n",
    "    df = make_label(df, cfg.pos_threshold)\n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"review_date\"])\n",
    "\n",
    "    cutoff = df[\"review_date\"].quantile(0.8)\n",
    "    df_train = df[df[\"review_date\"] <= cutoff].copy()\n",
    "    df_test  = df[df[\"review_date\"] >  cutoff].copy()\n",
    "\n",
    "    # ---- time split ----\n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"review_date\"])\n",
    "\n",
    "    cutoff = df[\"review_date\"].quantile(0.8)  # 前80%做train，后20%做test\n",
    "    df_train = df[df[\"review_date\"] <= cutoff].copy()\n",
    "    df_test  = df[df[\"review_date\"] >  cutoff].copy()\n",
    "\n",
    "    # 用户在 train 中见过的 items（需要从推荐列表里排除）\n",
    "    seen = (\n",
    "        df_train.groupby(\"user_id\")[\"business_id\"]\n",
    "        .apply(set)\n",
    "        .to_dict()\n",
    "    )\n",
    "    all_item_ids = items[\"business_id\"].astype(str).unique().tolist()\n",
    "\n",
    "    # 用户在 train 的正样本集合（采负样本时避免采到正样本）\n",
    "    user_pos = (\n",
    "        df_train[df_train[\"label\"] == 1]\n",
    "        .groupby(\"user_id\")[\"business_id\"]\n",
    "        .apply(set)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # ground truth 用 test 的正样本（更合理）\n",
    "    pos_test = df_test[df_test[\"label\"] == 1][[\"user_id\", \"business_id\"]].copy()\n",
    "    gt = pos_test.groupby(\"user_id\")[\"business_id\"].apply(list).to_dict()\n",
    "\n",
    "    print(\"cutoff:\", cutoff)\n",
    "    print(\"train rows:\", len(df_train), \"test rows:\", len(df_test))\n",
    "\n",
    "    # keep only feature columns that exist after merge\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    if len(feature_cols) == 0:\n",
    "        raise ValueError(\"No feature columns found in merged df. Check items csv columns.\")\n",
    "\n",
    "    user_profiles = build_user_profiles(df_train, feature_cols)\n",
    "    train_df, user_feature_cols = build_training_matrix(df_train, user_profiles, feature_cols)\n",
    "\n",
    "    X_user, X_item, y, scaler = fit_scaler_and_transform(train_df, feature_cols, user_feature_cols)\n",
    "    user_cache = {}\n",
    "    U = user_profiles[feature_cols].to_numpy(dtype=np.float32)  # (num_users, d)\n",
    "    U_df = pd.DataFrame(U, columns=feature_cols)\n",
    "    U_df = numeric_log1p_clip(U_df, feature_cols)\n",
    "    U_scaled = scaler.transform(U_df).astype(np.float32)\n",
    "\n",
    "    for idx, uid in enumerate(user_profiles.index):\n",
    "        user_cache[uid] = U_scaled[idx]\n",
    "    X_user_tr, X_user_te, X_item_tr, X_item_te, y_tr, y_te = train_test_split(\n",
    "        X_user, X_item, y,\n",
    "        test_size=cfg.test_size,\n",
    "        random_state=cfg.random_state,\n",
    "        stratify=y if len(np.unique(y)) > 1 else None\n",
    "    )\n",
    "\n",
    "    # train_loader = DataLoader(RecDataset(X_user_tr, X_item_tr, y_tr), batch_size=cfg.batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(RecDataset(X_user_te, X_item_te, y_te), batch_size=cfg.batch_size*2, shuffle=False)\n",
    "    # 只用 train 的正样本做 BPR 训练\n",
    "    df_pos_train = df_train[df_train[\"label\"] == 1][[\"user_id\", \"business_id\"]].copy()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        BPRDataset(df_pos_train, user_cache, feature_cols, items, scaler),\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # test 仍然可以用 RecDataset 做 AUC\n",
    "    test_loader = DataLoader(\n",
    "        RecDataset(X_user_te, X_item_te, y_te),\n",
    "        batch_size=cfg.batch_size*2,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(\"device:\", device)\n",
    "    model = TwoTower(input_dim=len(feature_cols), hidden_dim=cfg.hidden_dim, embed_dim=cfg.embed_dim).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    print(torch.backends.mps.is_available(), torch.backends.mps.is_built())\n",
    "    print(\"device:\", device)\n",
    "    print(\"train samples:\", len(X_user_tr), \"test samples:\", len(X_user_te))\n",
    "    print(\"num features:\", len(feature_cols))\n",
    "\n",
    "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = os.path.join(\"runs\", f\"twotower_{run_id}\")\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    writer.add_text(\n",
    "        \"hparams\",\n",
    "        f\"embed_dim={cfg.embed_dim}, hidden_dim={cfg.hidden_dim}, lr={cfg.lr}, \"\n",
    "        f\"wd={cfg.weight_decay}, bs={cfg.batch_size}\"\n",
    "    )\n",
    "\n",
    "    print(\"TensorBoard log_dir:\", log_dir)\n",
    "\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for step, (u_x, pos_i, neg_i) in enumerate(train_loader):\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Epoch {epoch} step {step} ...\")\n",
    "\n",
    "            u_x  = u_x.to(device, non_blocking=True).float()\n",
    "            pos_i = pos_i.to(device, non_blocking=True).float()\n",
    "            neg_i = neg_i.to(device, non_blocking=True).float()\n",
    "\n",
    "\n",
    "            pos_s = model(u_x, pos_i)\n",
    "            neg_s = model(u_x, neg_i)\n",
    "            loss = bpr_loss(pos_s, neg_s)\n",
    "            global_step = (epoch - 1) * len(train_loader) + step\n",
    "            if step % 50 == 0:\n",
    "                writer.add_scalar(\"loss/train_step\", loss.item(), global_step)\n",
    "\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * u_x.size(0)\n",
    "\n",
    "        # eval AUC\n",
    "        model.eval()\n",
    "        all_y, all_p = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb_user, xb_item, yb in test_loader:\n",
    "                xb_user = xb_user.to(device).float()\n",
    "                xb_item = xb_item.to(device).float()\n",
    "                yb = yb.to(device)\n",
    "                logits = model(xb_user, xb_item)\n",
    "                prob = torch.sigmoid(logits).cpu().numpy()\n",
    "                all_p.append(prob)\n",
    "                all_y.append(yb.cpu().numpy())\n",
    "        y_true = np.concatenate(all_y)\n",
    "        y_prob = np.concatenate(all_p)\n",
    "        auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={avg_loss:.4f} | test_AUC={auc:.4f}\")\n",
    "        writer.add_scalar(\"loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"metrics/test_auc\", auc, epoch)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # ---- export: model + scaler + metadata ----\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.artifacts_dir, \"twotower_model.pth\"))\n",
    "    joblib.dump(scaler, os.path.join(cfg.artifacts_dir, \"scaler.pkl\"))\n",
    "    save_metadata(cfg.artifacts_dir, feature_cols)\n",
    "    print(\"Saved model/scaler/feature_cols to\", cfg.artifacts_dir)\n",
    "\n",
    "    # ---- export: ALL item embeddings (unique restaurants) ----\n",
    "    item_ids, all_item_X = transform_all_items(items, feature_cols, scaler)\n",
    "    all_item_tensor = torch.from_numpy(all_item_X).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        item_emb = model.item_tower(all_item_tensor).cpu().numpy()\n",
    "\n",
    "    np.save(os.path.join(cfg.artifacts_dir, \"item_ids.npy\"), item_ids)\n",
    "    np.save(os.path.join(cfg.artifacts_dir, \"item_emb.npy\"), item_emb)\n",
    "    print(\"Saved item_ids.npy and item_emb.npy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff: 2019-02-11 02:51:09.200000\n",
      "train rows: 549146 test rows: 137287\n",
      "device: mps\n",
      "True True\n",
      "device: mps\n",
      "train samples: 404228 test samples: 101057\n",
      "num features: 54\n",
      "TensorBoard log_dir: runs/twotower_20260216-151112\n",
      "Epoch 1 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 50 ...\n",
      "Epoch 1 step 100 ...\n",
      "Epoch 1 step 150 ...\n",
      "Epoch 01 | train_loss=0.2240 | test_AUC=0.7915\n",
      "Epoch 2 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 50 ...\n",
      "Epoch 2 step 100 ...\n",
      "Epoch 2 step 150 ...\n",
      "Epoch 02 | train_loss=0.1601 | test_AUC=0.8082\n",
      "Epoch 3 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 50 ...\n",
      "Epoch 3 step 100 ...\n",
      "Epoch 3 step 150 ...\n",
      "Epoch 03 | train_loss=0.1424 | test_AUC=0.8161\n",
      "Epoch 4 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 50 ...\n",
      "Epoch 4 step 100 ...\n",
      "Epoch 4 step 150 ...\n",
      "Epoch 04 | train_loss=0.1323 | test_AUC=0.8196\n",
      "Epoch 5 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 50 ...\n",
      "Epoch 5 step 100 ...\n",
      "Epoch 5 step 150 ...\n",
      "Epoch 05 | train_loss=0.1270 | test_AUC=0.8233\n",
      "Epoch 6 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 50 ...\n",
      "Epoch 6 step 100 ...\n",
      "Epoch 6 step 150 ...\n",
      "Epoch 06 | train_loss=0.1241 | test_AUC=0.8256\n",
      "Epoch 7 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 50 ...\n",
      "Epoch 7 step 100 ...\n",
      "Epoch 7 step 150 ...\n",
      "Epoch 07 | train_loss=0.1186 | test_AUC=0.8344\n",
      "Epoch 8 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 step 50 ...\n",
      "Epoch 8 step 100 ...\n",
      "Epoch 8 step 150 ...\n",
      "Epoch 08 | train_loss=0.1163 | test_AUC=0.8329\n",
      "Epoch 9 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 step 50 ...\n",
      "Epoch 9 step 100 ...\n",
      "Epoch 9 step 150 ...\n",
      "Epoch 09 | train_loss=0.1140 | test_AUC=0.8355\n",
      "Epoch 10 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 step 50 ...\n",
      "Epoch 10 step 100 ...\n",
      "Epoch 10 step 150 ...\n",
      "Epoch 10 | train_loss=0.1114 | test_AUC=0.8412\n",
      "Epoch 11 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 step 50 ...\n",
      "Epoch 11 step 100 ...\n",
      "Epoch 11 step 150 ...\n",
      "Epoch 11 | train_loss=0.1093 | test_AUC=0.8383\n",
      "Epoch 12 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 step 50 ...\n",
      "Epoch 12 step 100 ...\n",
      "Epoch 12 step 150 ...\n",
      "Epoch 12 | train_loss=0.1084 | test_AUC=0.8406\n",
      "Epoch 13 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 step 50 ...\n",
      "Epoch 13 step 100 ...\n",
      "Epoch 13 step 150 ...\n",
      "Epoch 13 | train_loss=0.1068 | test_AUC=0.8436\n",
      "Epoch 14 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 step 50 ...\n",
      "Epoch 14 step 100 ...\n",
      "Epoch 14 step 150 ...\n",
      "Epoch 14 | train_loss=0.1051 | test_AUC=0.8341\n",
      "Epoch 15 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 step 50 ...\n",
      "Epoch 15 step 100 ...\n",
      "Epoch 15 step 150 ...\n",
      "Epoch 15 | train_loss=0.1050 | test_AUC=0.8415\n",
      "Epoch 16 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 step 50 ...\n",
      "Epoch 16 step 100 ...\n",
      "Epoch 16 step 150 ...\n",
      "Epoch 16 | train_loss=0.1022 | test_AUC=0.8472\n",
      "Epoch 17 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 step 50 ...\n",
      "Epoch 17 step 100 ...\n",
      "Epoch 17 step 150 ...\n",
      "Epoch 17 | train_loss=0.1013 | test_AUC=0.8445\n",
      "Epoch 18 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 step 50 ...\n",
      "Epoch 18 step 100 ...\n",
      "Epoch 18 step 150 ...\n",
      "Epoch 18 | train_loss=0.1007 | test_AUC=0.8493\n",
      "Epoch 19 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 step 50 ...\n",
      "Epoch 19 step 100 ...\n",
      "Epoch 19 step 150 ...\n",
      "Epoch 19 | train_loss=0.1004 | test_AUC=0.8476\n",
      "Epoch 20 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 step 50 ...\n",
      "Epoch 20 step 100 ...\n",
      "Epoch 20 step 150 ...\n",
      "Epoch 20 | train_loss=0.0989 | test_AUC=0.8459\n",
      "Epoch 21 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 step 50 ...\n",
      "Epoch 21 step 100 ...\n",
      "Epoch 21 step 150 ...\n",
      "Epoch 21 | train_loss=0.0998 | test_AUC=0.8478\n",
      "Epoch 22 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 step 50 ...\n",
      "Epoch 22 step 100 ...\n",
      "Epoch 22 step 150 ...\n",
      "Epoch 22 | train_loss=0.0983 | test_AUC=0.8539\n",
      "Epoch 23 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 step 50 ...\n",
      "Epoch 23 step 100 ...\n",
      "Epoch 23 step 150 ...\n",
      "Epoch 23 | train_loss=0.0964 | test_AUC=0.8539\n",
      "Epoch 24 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 step 50 ...\n",
      "Epoch 24 step 100 ...\n",
      "Epoch 24 step 150 ...\n",
      "Epoch 24 | train_loss=0.0953 | test_AUC=0.8488\n",
      "Epoch 25 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 step 50 ...\n",
      "Epoch 25 step 100 ...\n",
      "Epoch 25 step 150 ...\n",
      "Epoch 25 | train_loss=0.0951 | test_AUC=0.8484\n",
      "Epoch 26 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 step 50 ...\n",
      "Epoch 26 step 100 ...\n",
      "Epoch 26 step 150 ...\n",
      "Epoch 26 | train_loss=0.0945 | test_AUC=0.8557\n",
      "Epoch 27 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 step 50 ...\n",
      "Epoch 27 step 100 ...\n",
      "Epoch 27 step 150 ...\n",
      "Epoch 27 | train_loss=0.0931 | test_AUC=0.8501\n",
      "Epoch 28 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 step 50 ...\n",
      "Epoch 28 step 100 ...\n",
      "Epoch 28 step 150 ...\n",
      "Epoch 28 | train_loss=0.0933 | test_AUC=0.8517\n",
      "Epoch 29 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 step 50 ...\n",
      "Epoch 29 step 100 ...\n",
      "Epoch 29 step 150 ...\n",
      "Epoch 29 | train_loss=0.0933 | test_AUC=0.8529\n",
      "Epoch 30 step 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 step 50 ...\n",
      "Epoch 30 step 100 ...\n",
      "Epoch 30 step 150 ...\n",
      "Epoch 30 | train_loss=0.0920 | test_AUC=0.8540\n",
      "Saved model/scaler/feature_cols to artifacts\n",
      "Saved item_ids.npy and item_emb.npy\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "id": "5e1a51b84c875f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:16:04.338478Z",
     "start_time": "2026-02-16T21:16:03.645385Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "    artifacts = cfg.artifacts_dir\n",
    "\n",
    "    # load metadata\n",
    "    feature_cols = json.load(open(os.path.join(artifacts, \"feature_cols.json\")))\n",
    "    scaler = joblib.load(os.path.join(artifacts, \"scaler.pkl\"))\n",
    "\n",
    "    # load item embeddings\n",
    "    item_ids = np.load(os.path.join(artifacts, \"item_ids.npy\"), allow_pickle=True)\n",
    "    item_emb = np.load(os.path.join(artifacts, \"item_emb.npy\"))\n",
    "\n",
    "    # load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TwoTower(input_dim=len(feature_cols), hidden_dim=cfg.hidden_dim, embed_dim=cfg.embed_dim).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(artifacts, \"twotower_model.pth\"), map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # build user_profiles again (or you can save them too)\n",
    "    interactions, items = load_raw(cfg.interactions_path, cfg.items_path)\n",
    "    df = make_merged_df(interactions, items)\n",
    "    df = make_label(df, cfg.pos_threshold)\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    user_profiles = build_user_profiles(df, feature_cols)\n",
    "\n",
    "    # pick a user\n",
    "    user_id = user_profiles.index[0]  # 你也可以改成你指定的 user_id\n",
    "    u = user_profiles.loc[user_id, feature_cols].to_numpy(dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "    # same preprocess: numeric -> fill -> clip -> log1p -> scaler\n",
    "    u_df = pd.DataFrame(u, columns=feature_cols)\n",
    "    u_x = numeric_log1p_clip(u_df, feature_cols)\n",
    "    u_x = scaler.transform(u_x).astype(np.float32)\n",
    "\n",
    "    user_tensor = torch.from_numpy(u_x).to(device)\n",
    "    item_emb_t = torch.from_numpy(item_emb).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        user_vec = model.user_tower(user_tensor)              # [1, d]\n",
    "        scores = torch.matmul(item_emb_t, user_vec.T).squeeze(1)  # [num_items]\n",
    "        topk = torch.topk(scores, k=cfg.k)\n",
    "\n",
    "    rec_ids = item_ids[topk.indices.cpu().numpy()].tolist()\n",
    "\n",
    "    # 去重（保险）\n",
    "    rec_unique = []\n",
    "    for bid in rec_ids:\n",
    "        if bid not in rec_unique:\n",
    "            rec_unique.append(bid)\n",
    "    rec_unique = rec_unique[:cfg.k]\n",
    "\n",
    "    print(\"user_id:\", user_id)\n",
    "    print(\"Top-K business_id:\")\n",
    "    print(rec_unique)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: ---r61b7EpVPkb4UVme5tA\n",
      "Top-K business_id:\n",
      "['qsm7SkX60JsajT7Yz248FA', 'nIlmZLuMs0JuBRvAHSIf8Q', 'amgCBbemSn2b1zUOoUbjHA', 'LVfCt9n02ylrle12sP_-1w', '5UN1B7XqZohGuULLNlWL1A', 'jM-oH5U5zcnfpmcbsgMVHQ', 'K_CS--rB2jpdZ_YmffEInQ', 'Vvd12n0sYII8rUgPhq-XNA', 'j2m_DDgHRAjB0Vx3j8wepw', 'Zxi0AGG2Dh-SeqHEMtuUYg']\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "id": "66fbbaf534171278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:16:46.828224Z",
     "start_time": "2026-02-16T21:16:04.346918Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from math import log2\n",
    "\n",
    "\n",
    "def ndcg_at_k(hit_positions, k):\n",
    "    dcg = 0.0\n",
    "    for r in hit_positions:\n",
    "        if r <= k:\n",
    "            dcg += 1.0 / log2(r + 1)\n",
    "    ideal_hits = min(len(hit_positions), k)\n",
    "    idcg = sum(1.0 / log2(i + 2) for i in range(ideal_hits))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "    artifacts = cfg.artifacts_dir\n",
    "\n",
    "    # ===== load artifacts =====\n",
    "    feature_cols = json.load(open(os.path.join(artifacts, \"feature_cols.json\")))\n",
    "    scaler = joblib.load(os.path.join(artifacts, \"scaler.pkl\"))\n",
    "    item_ids = np.load(os.path.join(artifacts, \"item_ids.npy\"), allow_pickle=True)\n",
    "    item_emb = np.load(os.path.join(artifacts, \"item_emb.npy\"))\n",
    "\n",
    "    # ===== device (cuda > mps > cpu) =====\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # ===== load model =====\n",
    "    model = TwoTower(input_dim=len(feature_cols), hidden_dim=cfg.hidden_dim, embed_dim=cfg.embed_dim).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(artifacts, \"twotower_model.pth\"), map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # ===== rebuild df =====\n",
    "    interactions, items = load_raw(cfg.interactions_path, cfg.items_path)\n",
    "    df = make_merged_df(interactions, items)\n",
    "    df = make_label(df, cfg.pos_threshold)\n",
    "\n",
    "    # ===== rebuild time split (你要的这一段) =====\n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"review_date\"]).copy()\n",
    "\n",
    "    cutoff = df[\"review_date\"].quantile(0.8)\n",
    "    df_train = df[df[\"review_date\"] <= cutoff].copy()\n",
    "    df_test  = df[df[\"review_date\"] >  cutoff].copy()\n",
    "\n",
    "    # ===== align feature cols =====\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "\n",
    "    # ===== user_profiles from TRAIN only =====\n",
    "    user_profiles = build_user_profiles(df_train, feature_cols)\n",
    "\n",
    "    # ===== seen from TRAIN only =====\n",
    "    seen = (\n",
    "        df_train.groupby(\"user_id\")[\"business_id\"]\n",
    "        .apply(set)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # ===== ground truth from TEST positives only =====\n",
    "    pos_test = df_test[df_test[\"label\"] == 1][[\"user_id\", \"business_id\"]].copy()\n",
    "    gt = pos_test.groupby(\"user_id\")[\"business_id\"].apply(set).to_dict()\n",
    "\n",
    "    # ===== item embedding tensor =====\n",
    "    item_emb_t = torch.from_numpy(item_emb).to(device)\n",
    "\n",
    "    recalls, ndcgs = [], []\n",
    "\n",
    "    users = list(user_profiles.index)\n",
    "    users = [u for u in gt.keys() if u in user_profiles.index]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for uid in users:\n",
    "            true_items = gt.get(uid, set()) - seen.get(uid, set())\n",
    "            if not true_items:\n",
    "                continue\n",
    "\n",
    "            # user vector input (same preprocess as training)\n",
    "            u = user_profiles.loc[uid, feature_cols].to_numpy(dtype=np.float32).reshape(1, -1)\n",
    "            u_df = pd.DataFrame(u, columns=feature_cols)\n",
    "            u_x = numeric_log1p_clip(u_df, feature_cols)\n",
    "            u_x = scaler.transform(u_x).astype(np.float32)\n",
    "            user_tensor = torch.from_numpy(u_x).to(device)\n",
    "\n",
    "            user_vec = model.user_tower(user_tensor)              # [1, d]\n",
    "            scores = torch.matmul(item_emb_t, user_vec.T).squeeze(1)  # [num_items]\n",
    "\n",
    "            # ===== mask seen BEFORE topk =====\n",
    "            seen_set = seen.get(uid, set())\n",
    "            if seen_set:\n",
    "                mask = np.isin(item_ids, np.array(list(seen_set), dtype=object))\n",
    "                if mask.any():\n",
    "                    scores = scores.masked_fill(torch.from_numpy(mask).to(device), float(\"-inf\"))\n",
    "\n",
    "            topk = torch.topk(scores, k=cfg.k)\n",
    "            rec = item_ids[topk.indices.detach().cpu().numpy()].tolist()\n",
    "\n",
    "            hits = [i for i, bid in enumerate(rec, start=1) if bid in true_items]\n",
    "            denom = min(len(true_items), cfg.k)\n",
    "            recall = (len(hits) / denom) if denom > 0 else 0.0\n",
    "\n",
    "            recalls.append(recall)\n",
    "            ndcgs.append(ndcg_at_k(hits, cfg.k))\n",
    "\n",
    "    print(f\"cutoff: {cutoff}\")\n",
    "    print(f\"Users evaluated: {len(recalls)}\")\n",
    "    print(f\"Recall@{cfg.k}: {float(np.mean(recalls)):.4f}\")\n",
    "    print(f\"NDCG@{cfg.k}:  {float(np.mean(ndcgs)):.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff: 2019-02-11 02:51:09.200000\n",
      "Users evaluated: 11269\n",
      "Recall@10: 0.0154\n",
      "NDCG@10:  0.0230\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tensorboard --logdir runs --port 6006",
   "id": "1e6251db0341c13e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
