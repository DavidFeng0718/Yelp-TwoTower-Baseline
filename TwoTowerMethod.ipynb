{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T01:44:07.495293Z",
     "start_time": "2026-02-16T01:44:07.470629Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    interactions_path: str = \"data/cb_baseline.parquet\"\n",
    "    items_path: str = \"data/geo_feature_matrix.csv\"\n",
    "    artifacts_dir: str = \"artifacts\"\n",
    "\n",
    "    embed_dim: int = 128\n",
    "    hidden_dim: int =512\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    epochs: int = 30 #10\n",
    "    batch_size: int = 2048 #2048\n",
    "\n",
    "    # label rule\n",
    "    # centered_rating > 0 => positive\n",
    "    pos_threshold: float = 0.0\n",
    "\n",
    "    # evaluation    \n",
    "    k: int = 20\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3cdb7d43e2d887f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T01:44:07.515067Z",
     "start_time": "2026-02-16T01:44:07.504478Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TwoTower(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128, embed_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_x, item_x):\n",
    "        u = self.user_tower(user_x)\n",
    "        i = self.item_tower(item_x)\n",
    "        # dot product\n",
    "        return (u * i).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0a6354c2ade3207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T01:44:07.529644Z",
     "start_time": "2026-02-16T01:44:07.518719Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_raw(interactions_path: str, items_path: str):\n",
    "    interactions = pd.read_parquet(interactions_path)\n",
    "    items = pd.read_csv(items_path)\n",
    "    def load_raw(interactions_path: str, items_path: str):\n",
    "        interactions = pd.read_parquet(interactions_path)\n",
    "        items = pd.read_csv(items_path)\n",
    "\n",
    "        # ✅ 关键：清理列名空格/不可见字符，避免 merge / 筛列失败\n",
    "        interactions.columns = interactions.columns.str.strip()\n",
    "        items.columns = items.columns.str.strip()\n",
    "\n",
    "        # ✅ 快速自检：items到底有多少列\n",
    "        print(\"[DEBUG] items shape:\", items.shape)\n",
    "        print(\"[DEBUG] first 30 item columns:\", list(items.columns[:30]))\n",
    "        print(\"[DEBUG] last  30 item columns:\", list(items.columns[-30:]))\n",
    "\n",
    "        interactions[\"business_id\"] = interactions[\"business_id\"].astype(str)\n",
    "        items[\"business_id\"] = items[\"business_id\"].astype(str)\n",
    "        interactions[\"user_id\"] = interactions[\"user_id\"].astype(str)\n",
    "\n",
    "        return interactions, items\n",
    "\n",
    "\n",
    "    # unify types\n",
    "    interactions[\"business_id\"] = interactions[\"business_id\"].astype(str)\n",
    "    items[\"business_id\"] = items[\"business_id\"].astype(str)\n",
    "    interactions[\"user_id\"] = interactions[\"user_id\"].astype(str)\n",
    "\n",
    "    return interactions, items\n",
    "\n",
    "def get_feature_cols(items: pd.DataFrame):\n",
    "    # 先排除 id\n",
    "    candidate = items.drop(columns=[\"business_id\"], errors=\"ignore\")\n",
    "\n",
    "    # 只保留“看起来像数值”的列：要么本来就是数值 dtype，\n",
    "    # 要么能被安全转换成数值（比如object但内容是 \"25.0\"）\n",
    "    numeric_cols = []\n",
    "    for c in candidate.columns:\n",
    "        s = pd.to_numeric(candidate[c], errors=\"coerce\")\n",
    "        # 至少有一部分不是 NaN 才算有效特征\n",
    "        if s.notna().mean() > 0.5:\n",
    "            numeric_cols.append(c)\n",
    "\n",
    "    print(\"[DEBUG] feature cols:\", len(numeric_cols))\n",
    "    return numeric_cols\n",
    "\n",
    "\n",
    "def make_merged_df(interactions: pd.DataFrame, items: pd.DataFrame):\n",
    "    df = interactions.merge(items, on=\"business_id\", how=\"inner\")\n",
    "    return df\n",
    "\n",
    "def make_label(df: pd.DataFrame, pos_threshold: float = 0.0):\n",
    "    if \"centered_rating\" not in df.columns:\n",
    "        raise ValueError(\"cb_baseline.parquet must contain centered_rating column.\")\n",
    "    df[\"label\"] = (df[\"centered_rating\"] > pos_threshold).astype(int)\n",
    "    return df\n",
    "\n",
    "def numeric_log1p_clip(df: pd.DataFrame, cols):\n",
    "    x = df[cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    x = x.clip(lower=0.0)\n",
    "    return np.log1p(x.to_numpy(dtype=np.float32))\n",
    "\n",
    "\n",
    "def build_user_profiles(df: pd.DataFrame, feature_cols):\n",
    "    # user profile = mean of positive items' content features\n",
    "    pos = df[df[\"label\"] == 1].copy()\n",
    "    if len(pos) == 0:\n",
    "        raise ValueError(\"No positive samples found. Check pos_threshold or centered_rating distribution.\")\n",
    "    for c in feature_cols:\n",
    "        pos[c] = pd.to_numeric(pos[c], errors=\"coerce\")\n",
    "    pos[feature_cols] = pos[feature_cols].fillna(0.0)\n",
    "    profiles = pos.groupby(\"user_id\")[feature_cols].mean()\n",
    "    return profiles\n",
    "\n",
    "def build_training_matrix(df: pd.DataFrame, user_profiles: pd.DataFrame, feature_cols):\n",
    "    # attach user_profile columns to each interaction row\n",
    "    train_df = df.merge(user_profiles, on=\"user_id\", how=\"inner\", suffixes=(\"\", \"_user\"))\n",
    "    if len(train_df) == 0:\n",
    "        raise ValueError(\"train_df is empty after merging user_profiles. Possibly no users with positives in df.\")\n",
    "    user_feature_cols = [f\"{c}_user\" for c in feature_cols]\n",
    "    return train_df, user_feature_cols\n",
    "\n",
    "def fit_scaler_and_transform(train_df: pd.DataFrame, feature_cols, user_feature_cols):\n",
    "    X_item = numeric_log1p_clip(train_df, feature_cols)\n",
    "    X_user = numeric_log1p_clip(train_df, user_feature_cols)\n",
    "    y = train_df[\"label\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_item = scaler.fit_transform(X_item).astype(np.float32)\n",
    "    X_user = scaler.transform(X_user).astype(np.float32)\n",
    "    return X_user, X_item, y, scaler\n",
    "\n",
    "def transform_all_items(items: pd.DataFrame, feature_cols, scaler: StandardScaler):\n",
    "    # IMPORTANT: recommend over UNIQUE restaurants, not interaction rows\n",
    "    tmp = items[[\"business_id\"] + feature_cols].copy()\n",
    "    X_item = numeric_log1p_clip(tmp, feature_cols)\n",
    "    X_item = scaler.transform(X_item).astype(np.float32)\n",
    "    item_ids = tmp[\"business_id\"].to_numpy()\n",
    "    return item_ids, X_item\n",
    "\n",
    "def save_metadata(artifacts_dir: str, feature_cols):\n",
    "    os.makedirs(artifacts_dir, exist_ok=True)\n",
    "    with open(os.path.join(artifacts_dir, \"feature_cols.json\"), \"w\") as f:\n",
    "        json.dump(feature_cols, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f57a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:05:38) [MSC v.1929 64 bit (AMD64)]\n",
      "torch: 2.10.0+cu128\n",
      "torch cuda: 12.8\n",
      "cuda available: True\n",
      "device count: 1\n",
      "gpu: NVIDIA GeForce RTX 5070\n",
      "archs: ['sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_100', 'sm_120']\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "print(\"python:\", sys.version)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torch cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "    print(\"archs:\", torch.cuda.get_arch_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71ceaf2c0012d244",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-16T01:44:07.530261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] feature cols: 54\n",
      "cutoff: 2019-02-11 02:51:09.200000\n",
      "train rows: 549146 test rows: 137287\n",
      "device: cuda\n",
      "False False\n",
      "device: cuda\n",
      "train samples: 404228 test samples: 101057\n",
      "num features: 54\n",
      "TensorBoard log_dir: runs\\twotower_20260215-223041\n",
      "Epoch 1 step 0 ...\n",
      "Epoch 1 step 50 ...\n",
      "Epoch 1 step 100 ...\n",
      "Epoch 1 step 150 ...\n",
      "Epoch 01 | train_loss=0.2287 | test_AUC=0.7938\n",
      "Epoch 2 step 0 ...\n",
      "Epoch 2 step 50 ...\n",
      "Epoch 2 step 100 ...\n",
      "Epoch 2 step 150 ...\n",
      "Epoch 02 | train_loss=0.1615 | test_AUC=0.8080\n",
      "Epoch 3 step 0 ...\n",
      "Epoch 3 step 50 ...\n",
      "Epoch 3 step 100 ...\n",
      "Epoch 3 step 150 ...\n",
      "Epoch 03 | train_loss=0.1455 | test_AUC=0.8139\n",
      "Epoch 4 step 0 ...\n",
      "Epoch 4 step 50 ...\n",
      "Epoch 4 step 100 ...\n",
      "Epoch 4 step 150 ...\n",
      "Epoch 04 | train_loss=0.1333 | test_AUC=0.8218\n",
      "Epoch 5 step 0 ...\n",
      "Epoch 5 step 50 ...\n",
      "Epoch 5 step 100 ...\n",
      "Epoch 5 step 150 ...\n",
      "Epoch 05 | train_loss=0.1295 | test_AUC=0.8255\n",
      "Epoch 6 step 0 ...\n",
      "Epoch 6 step 50 ...\n",
      "Epoch 6 step 100 ...\n",
      "Epoch 6 step 150 ...\n",
      "Epoch 06 | train_loss=0.1234 | test_AUC=0.8302\n",
      "Epoch 7 step 0 ...\n",
      "Epoch 7 step 50 ...\n",
      "Epoch 7 step 100 ...\n",
      "Epoch 7 step 150 ...\n",
      "Epoch 07 | train_loss=0.1199 | test_AUC=0.8247\n",
      "Epoch 8 step 0 ...\n",
      "Epoch 8 step 50 ...\n",
      "Epoch 8 step 100 ...\n",
      "Epoch 8 step 150 ...\n",
      "Epoch 08 | train_loss=0.1154 | test_AUC=0.8306\n",
      "Epoch 9 step 0 ...\n",
      "Epoch 9 step 50 ...\n",
      "Epoch 9 step 100 ...\n",
      "Epoch 9 step 150 ...\n",
      "Epoch 09 | train_loss=0.1135 | test_AUC=0.8309\n",
      "Epoch 10 step 0 ...\n",
      "Epoch 10 step 50 ...\n",
      "Epoch 10 step 100 ...\n",
      "Epoch 10 step 150 ...\n",
      "Epoch 10 | train_loss=0.1117 | test_AUC=0.8315\n",
      "Epoch 11 step 0 ...\n",
      "Epoch 11 step 50 ...\n",
      "Epoch 11 step 100 ...\n",
      "Epoch 11 step 150 ...\n",
      "Epoch 11 | train_loss=0.1104 | test_AUC=0.8373\n",
      "Epoch 12 step 0 ...\n",
      "Epoch 12 step 50 ...\n",
      "Epoch 12 step 100 ...\n",
      "Epoch 12 step 150 ...\n",
      "Epoch 12 | train_loss=0.1078 | test_AUC=0.8418\n",
      "Epoch 13 step 0 ...\n",
      "Epoch 13 step 50 ...\n",
      "Epoch 13 step 100 ...\n",
      "Epoch 13 step 150 ...\n",
      "Epoch 13 | train_loss=0.1045 | test_AUC=0.8430\n",
      "Epoch 14 step 0 ...\n",
      "Epoch 14 step 50 ...\n",
      "Epoch 14 step 100 ...\n",
      "Epoch 14 step 150 ...\n",
      "Epoch 14 | train_loss=0.1057 | test_AUC=0.8420\n",
      "Epoch 15 step 0 ...\n",
      "Epoch 15 step 50 ...\n",
      "Epoch 15 step 100 ...\n",
      "Epoch 15 step 150 ...\n",
      "Epoch 15 | train_loss=0.1032 | test_AUC=0.8385\n",
      "Epoch 16 step 0 ...\n",
      "Epoch 16 step 50 ...\n",
      "Epoch 16 step 100 ...\n",
      "Epoch 16 step 150 ...\n",
      "Epoch 16 | train_loss=0.1020 | test_AUC=0.8400\n",
      "Epoch 17 step 0 ...\n",
      "Epoch 17 step 50 ...\n",
      "Epoch 17 step 100 ...\n",
      "Epoch 17 step 150 ...\n",
      "Epoch 17 | train_loss=0.0999 | test_AUC=0.8459\n",
      "Epoch 18 step 0 ...\n",
      "Epoch 18 step 50 ...\n",
      "Epoch 18 step 100 ...\n",
      "Epoch 18 step 150 ...\n",
      "Epoch 18 | train_loss=0.1016 | test_AUC=0.8441\n",
      "Epoch 19 step 0 ...\n",
      "Epoch 19 step 50 ...\n",
      "Epoch 19 step 100 ...\n",
      "Epoch 19 step 150 ...\n",
      "Epoch 19 | train_loss=0.1003 | test_AUC=0.8446\n",
      "Epoch 20 step 0 ...\n",
      "Epoch 20 step 50 ...\n",
      "Epoch 20 step 100 ...\n",
      "Epoch 20 step 150 ...\n",
      "Epoch 20 | train_loss=0.0976 | test_AUC=0.8463\n",
      "Epoch 21 step 0 ...\n",
      "Epoch 21 step 50 ...\n",
      "Epoch 21 step 100 ...\n",
      "Epoch 21 step 150 ...\n",
      "Epoch 21 | train_loss=0.0970 | test_AUC=0.8471\n",
      "Epoch 22 step 0 ...\n",
      "Epoch 22 step 50 ...\n",
      "Epoch 22 step 100 ...\n",
      "Epoch 22 step 150 ...\n",
      "Epoch 22 | train_loss=0.0976 | test_AUC=0.8472\n",
      "Epoch 23 step 0 ...\n",
      "Epoch 23 step 50 ...\n",
      "Epoch 23 step 100 ...\n",
      "Epoch 23 step 150 ...\n",
      "Epoch 23 | train_loss=0.0960 | test_AUC=0.8465\n",
      "Epoch 24 step 0 ...\n",
      "Epoch 24 step 50 ...\n",
      "Epoch 24 step 100 ...\n",
      "Epoch 24 step 150 ...\n",
      "Epoch 24 | train_loss=0.0959 | test_AUC=0.8490\n",
      "Epoch 25 step 0 ...\n",
      "Epoch 25 step 50 ...\n",
      "Epoch 25 step 100 ...\n",
      "Epoch 25 step 150 ...\n",
      "Epoch 25 | train_loss=0.0952 | test_AUC=0.8477\n",
      "Epoch 26 step 0 ...\n",
      "Epoch 26 step 50 ...\n",
      "Epoch 26 step 100 ...\n",
      "Epoch 26 step 150 ...\n",
      "Epoch 26 | train_loss=0.0951 | test_AUC=0.8463\n",
      "Epoch 27 step 0 ...\n",
      "Epoch 27 step 50 ...\n",
      "Epoch 27 step 100 ...\n",
      "Epoch 27 step 150 ...\n",
      "Epoch 27 | train_loss=0.0944 | test_AUC=0.8465\n",
      "Epoch 28 step 0 ...\n",
      "Epoch 28 step 50 ...\n",
      "Epoch 28 step 100 ...\n",
      "Epoch 28 step 150 ...\n",
      "Epoch 28 | train_loss=0.0937 | test_AUC=0.8455\n",
      "Epoch 29 step 0 ...\n",
      "Epoch 29 step 50 ...\n",
      "Epoch 29 step 100 ...\n",
      "Epoch 29 step 150 ...\n",
      "Epoch 29 | train_loss=0.0930 | test_AUC=0.8504\n",
      "Epoch 30 step 0 ...\n",
      "Epoch 30 step 50 ...\n",
      "Epoch 30 step 100 ...\n",
      "Epoch 30 step 150 ...\n",
      "Epoch 30 | train_loss=0.0932 | test_AUC=0.8476\n",
      "Saved model/scaler/feature_cols to artifacts\n",
      "Saved item_ids.npy and item_emb.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import random\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, df_pos, user_cache, feature_cols, items_df, scaler):\n",
    "        self.df_pos = df_pos[[\"user_id\",\"business_id\"]].reset_index(drop=True)\n",
    "        self.user_cache = user_cache\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "        # 预处理好的“全量item特征矩阵”\n",
    "        self.item_ids, self.item_X = transform_all_items(items_df, feature_cols, scaler)\n",
    "        self.id2idx = {bid:i for i,bid in enumerate(self.item_ids)}\n",
    "        self.all_item_ids = list(self.id2idx.keys())\n",
    "\n",
    "        self.user_pos = (\n",
    "            df_pos.groupby(\"user_id\")[\"business_id\"].apply(set).to_dict()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid = self.df_pos.loc[idx, \"user_id\"]\n",
    "        pos_bid = self.df_pos.loc[idx, \"business_id\"]\n",
    "\n",
    "        # user vector：直接缓存取\n",
    "        u_x = self.user_cache[uid].astype(np.float32)\n",
    "\n",
    "        # pos item\n",
    "        pos_i = self.item_X[self.id2idx[pos_bid]]\n",
    "\n",
    "        # sample neg item\n",
    "        while True:\n",
    "            neg_bid = random.choice(self.all_item_ids)\n",
    "            if neg_bid not in self.user_pos.get(uid, set()):\n",
    "                break\n",
    "        neg_i = self.item_X[self.id2idx[neg_bid]]\n",
    "\n",
    "        return torch.from_numpy(u_x), torch.from_numpy(pos_i), torch.from_numpy(neg_i)\n",
    "\n",
    "\n",
    "class RecDataset(Dataset):\n",
    "    def __init__(self, X_user, X_item, y):\n",
    "        self.X_user = torch.from_numpy(X_user)\n",
    "        self.X_item = torch.from_numpy(X_item)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_user[idx], self.X_item[idx], self.y[idx]\n",
    "\n",
    "def bpr_loss(pos_score, neg_score):\n",
    "    return -torch.log(torch.sigmoid(pos_score - neg_score) + 1e-8).mean()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    cfg = Config()\n",
    "    os.makedirs(cfg.artifacts_dir, exist_ok=True)\n",
    "\n",
    "    interactions, items = load_raw(cfg.interactions_path, cfg.items_path)\n",
    "    feature_cols = get_feature_cols(items)\n",
    "\n",
    "    df = make_merged_df(interactions, items)\n",
    "    df = make_label(df, cfg.pos_threshold)\n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"review_date\"])\n",
    "\n",
    "    cutoff = df[\"review_date\"].quantile(0.8)\n",
    "    df_train = df[df[\"review_date\"] <= cutoff].copy()\n",
    "    df_test  = df[df[\"review_date\"] >  cutoff].copy()\n",
    "\n",
    "    # ---- time split ----\n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"review_date\"])\n",
    "\n",
    "    cutoff = df[\"review_date\"].quantile(0.8)  # 前80%做train，后20%做test\n",
    "    df_train = df[df[\"review_date\"] <= cutoff].copy()\n",
    "    df_test  = df[df[\"review_date\"] >  cutoff].copy()\n",
    "\n",
    "    # 用户在 train 中见过的 items（需要从推荐列表里排除）\n",
    "    seen = (\n",
    "        df_train.groupby(\"user_id\")[\"business_id\"]\n",
    "        .apply(set)\n",
    "        .to_dict()\n",
    "    )\n",
    "    all_item_ids = items[\"business_id\"].astype(str).unique().tolist()\n",
    "\n",
    "    # 用户在 train 的正样本集合（采负样本时避免采到正样本）\n",
    "    user_pos = (\n",
    "        df_train[df_train[\"label\"] == 1]\n",
    "        .groupby(\"user_id\")[\"business_id\"]\n",
    "        .apply(set)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # ground truth 用 test 的正样本（更合理）\n",
    "    pos_test = df_test[df_test[\"label\"] == 1][[\"user_id\", \"business_id\"]].copy()\n",
    "    gt = pos_test.groupby(\"user_id\")[\"business_id\"].apply(list).to_dict()\n",
    "\n",
    "    print(\"cutoff:\", cutoff)\n",
    "    print(\"train rows:\", len(df_train), \"test rows:\", len(df_test))\n",
    "\n",
    "    # keep only feature columns that exist after merge\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    if len(feature_cols) == 0:\n",
    "        raise ValueError(\"No feature columns found in merged df. Check items csv columns.\")\n",
    "\n",
    "    user_profiles = build_user_profiles(df_train, feature_cols)\n",
    "    train_df, user_feature_cols = build_training_matrix(df_train, user_profiles, feature_cols)\n",
    "\n",
    "    X_user, X_item, y, scaler = fit_scaler_and_transform(train_df, feature_cols, user_feature_cols)\n",
    "    user_cache = {}\n",
    "    U = user_profiles[feature_cols].to_numpy(dtype=np.float32)  # (num_users, d)\n",
    "    U_df = pd.DataFrame(U, columns=feature_cols)\n",
    "    U_df = numeric_log1p_clip(U_df, feature_cols)\n",
    "    U_scaled = scaler.transform(U_df).astype(np.float32)\n",
    "\n",
    "    for idx, uid in enumerate(user_profiles.index):\n",
    "        user_cache[uid] = U_scaled[idx]\n",
    "    X_user_tr, X_user_te, X_item_tr, X_item_te, y_tr, y_te = train_test_split(\n",
    "        X_user, X_item, y,\n",
    "        test_size=cfg.test_size,\n",
    "        random_state=cfg.random_state,\n",
    "        stratify=y if len(np.unique(y)) > 1 else None\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(RecDataset(X_user_tr, X_item_tr, y_tr), batch_size=cfg.batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(RecDataset(X_user_te, X_item_te, y_te), batch_size=cfg.batch_size*2, shuffle=False)\n",
    "    # 只用 train 的正样本做 BPR 训练\n",
    "    df_pos_train = df_train[df_train[\"label\"] == 1][[\"user_id\", \"business_id\"]].copy()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        BPRDataset(df_pos_train, user_cache, feature_cols, items, scaler),\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # test 仍然可以用 RecDataset 做 AUC（可选）\n",
    "    test_loader = DataLoader(\n",
    "        RecDataset(X_user_te, X_item_te, y_te),\n",
    "        batch_size=cfg.batch_size*2,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(\"device:\", device)\n",
    "    model = TwoTower(input_dim=len(feature_cols), hidden_dim=cfg.hidden_dim, embed_dim=cfg.embed_dim).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    print(torch.backends.mps.is_available(), torch.backends.mps.is_built())\n",
    "    print(\"device:\", device)\n",
    "    print(\"train samples:\", len(X_user_tr), \"test samples:\", len(X_user_te))\n",
    "    print(\"num features:\", len(feature_cols))\n",
    "\n",
    "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = os.path.join(\"runs\", f\"twotower_{run_id}\")\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    writer.add_text(\n",
    "        \"hparams\",\n",
    "        f\"embed_dim={cfg.embed_dim}, hidden_dim={cfg.hidden_dim}, lr={cfg.lr}, \"\n",
    "        f\"wd={cfg.weight_decay}, bs={cfg.batch_size}\"\n",
    "    )\n",
    "\n",
    "    print(\"TensorBoard log_dir:\", log_dir)\n",
    "\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for step, (u_x, pos_i, neg_i) in enumerate(train_loader):\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Epoch {epoch} step {step} ...\")\n",
    "\n",
    "            u_x  = u_x.to(device, non_blocking=True).float()\n",
    "            pos_i = pos_i.to(device, non_blocking=True).float()\n",
    "            neg_i = neg_i.to(device, non_blocking=True).float()\n",
    "\n",
    "\n",
    "            pos_s = model(u_x, pos_i)\n",
    "            neg_s = model(u_x, neg_i)\n",
    "            loss = bpr_loss(pos_s, neg_s)\n",
    "            global_step = (epoch - 1) * len(train_loader) + step\n",
    "            if step % 50 == 0:\n",
    "                writer.add_scalar(\"loss/train_step\", loss.item(), global_step)\n",
    "\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * u_x.size(0)\n",
    "\n",
    "        # eval AUC\n",
    "        model.eval()\n",
    "        all_y, all_p = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb_user, xb_item, yb in test_loader:\n",
    "                xb_user = xb_user.to(device).float()\n",
    "                xb_item = xb_item.to(device).float()\n",
    "                yb = yb.to(device)\n",
    "                logits = model(xb_user, xb_item)\n",
    "                prob = torch.sigmoid(logits).cpu().numpy()\n",
    "                all_p.append(prob)\n",
    "                all_y.append(yb.cpu().numpy())\n",
    "        y_true = np.concatenate(all_y)\n",
    "        y_prob = np.concatenate(all_p)\n",
    "        auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={avg_loss:.4f} | test_AUC={auc:.4f}\")\n",
    "        writer.add_scalar(\"loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"metrics/test_auc\", auc, epoch)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # ---- export: model + scaler + metadata ----\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.artifacts_dir, \"twotower_model.pth\"))\n",
    "    joblib.dump(scaler, os.path.join(cfg.artifacts_dir, \"scaler.pkl\"))\n",
    "    save_metadata(cfg.artifacts_dir, feature_cols)\n",
    "    print(\"Saved model/scaler/feature_cols to\", cfg.artifacts_dir)\n",
    "\n",
    "    # ---- export: ALL item embeddings (unique restaurants) ----\n",
    "    item_ids, all_item_X = transform_all_items(items, feature_cols, scaler)\n",
    "    all_item_tensor = torch.from_numpy(all_item_X).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        item_emb = model.item_tower(all_item_tensor).cpu().numpy()\n",
    "\n",
    "    np.save(os.path.join(cfg.artifacts_dir, \"item_ids.npy\"), item_ids)\n",
    "    np.save(os.path.join(cfg.artifacts_dir, \"item_emb.npy\"), item_emb)\n",
    "    print(\"Saved item_ids.npy and item_emb.npy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e1a51b84c875f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: ---r61b7EpVPkb4UVme5tA\n",
      "Top-K business_id:\n",
      "['qsm7SkX60JsajT7Yz248FA', 'Zxi0AGG2Dh-SeqHEMtuUYg', 'Vvd12n0sYII8rUgPhq-XNA', 'j2m_DDgHRAjB0Vx3j8wepw', 'P3vUpcO1EPPmXOZY-Wjdrw', 'e4InIycH2PAJWccBBj0tAA', 'RxrgGdjVD5fFY7DnJoj3tg', 'oB2B1KlyhlY5LTy18jRqzw', 'QWqKTWQ2OiDgo3dzNkpung', 'vA5MHe9LqNQcp8k_CaFkHg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "    artifacts = cfg.artifacts_dir\n",
    "\n",
    "    # load metadata\n",
    "    feature_cols = json.load(open(os.path.join(artifacts, \"feature_cols.json\")))\n",
    "    scaler = joblib.load(os.path.join(artifacts, \"scaler.pkl\"))\n",
    "\n",
    "    # load item embeddings\n",
    "    item_ids = np.load(os.path.join(artifacts, \"item_ids.npy\"), allow_pickle=True)\n",
    "    item_emb = np.load(os.path.join(artifacts, \"item_emb.npy\"))\n",
    "\n",
    "    # load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TwoTower(input_dim=len(feature_cols), hidden_dim=cfg.hidden_dim, embed_dim=cfg.embed_dim).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(artifacts, \"twotower_model.pth\"), map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # build user_profiles again (or you can save them too)\n",
    "    interactions, items = load_raw(cfg.interactions_path, cfg.items_path)\n",
    "    df = make_merged_df(interactions, items)\n",
    "    df = make_label(df, cfg.pos_threshold)\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    user_profiles = build_user_profiles(df, feature_cols)\n",
    "\n",
    "    # pick a user\n",
    "    user_id = user_profiles.index[0]  # 你也可以改成你指定的 user_id\n",
    "    u = user_profiles.loc[user_id, feature_cols].to_numpy(dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "    # same preprocess: numeric -> fill -> clip -> log1p -> scaler\n",
    "    u_df = pd.DataFrame(u, columns=feature_cols)\n",
    "    u_x = numeric_log1p_clip(u_df, feature_cols)\n",
    "    u_x = scaler.transform(u_x).astype(np.float32)\n",
    "\n",
    "    user_tensor = torch.from_numpy(u_x).to(device)\n",
    "    item_emb_t = torch.from_numpy(item_emb).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        user_vec = model.user_tower(user_tensor)              # [1, d]\n",
    "        scores = torch.matmul(item_emb_t, user_vec.T).squeeze(1)  # [num_items]\n",
    "        topk = torch.topk(scores, k=cfg.k)\n",
    "\n",
    "    rec_ids = item_ids[topk.indices.cpu().numpy()].tolist()\n",
    "\n",
    "    # 去重（保险）\n",
    "    rec_unique = []\n",
    "    for bid in rec_ids:\n",
    "        if bid not in rec_unique:\n",
    "            rec_unique.append(bid)\n",
    "    rec_unique = rec_unique[:cfg.k]\n",
    "\n",
    "    print(\"user_id:\", user_id)\n",
    "    print(\"Top-K business_id:\")\n",
    "    print(rec_unique)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66fbbaf534171278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff: 2019-02-11 02:51:09.200000\n",
      "Users evaluated: 11269\n",
      "Recall@10: 0.0163\n",
      "NDCG@10:  0.0234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from math import log2\n",
    "\n",
    "\n",
    "def ndcg_at_k(hit_positions, k):\n",
    "    dcg = 0.0\n",
    "    for r in hit_positions:\n",
    "        if r <= k:\n",
    "            dcg += 1.0 / log2(r + 1)\n",
    "    ideal_hits = min(len(hit_positions), k)\n",
    "    idcg = sum(1.0 / log2(i + 2) for i in range(ideal_hits))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "    artifacts = cfg.artifacts_dir\n",
    "\n",
    "    # ===== load artifacts =====\n",
    "    feature_cols = json.load(open(os.path.join(artifacts, \"feature_cols.json\")))\n",
    "    scaler = joblib.load(os.path.join(artifacts, \"scaler.pkl\"))\n",
    "    item_ids = np.load(os.path.join(artifacts, \"item_ids.npy\"), allow_pickle=True)\n",
    "    item_emb = np.load(os.path.join(artifacts, \"item_emb.npy\"))\n",
    "\n",
    "    # ===== device (cuda > mps > cpu) =====\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # ===== load model =====\n",
    "    model = TwoTower(input_dim=len(feature_cols), hidden_dim=cfg.hidden_dim, embed_dim=cfg.embed_dim).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(artifacts, \"twotower_model.pth\"), map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # ===== rebuild df =====\n",
    "    interactions, items = load_raw(cfg.interactions_path, cfg.items_path)\n",
    "    df = make_merged_df(interactions, items)\n",
    "    df = make_label(df, cfg.pos_threshold)\n",
    "\n",
    "    # ===== rebuild time split (你要的这一段) =====\n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"review_date\"]).copy()\n",
    "\n",
    "    cutoff = df[\"review_date\"].quantile(0.8)\n",
    "    df_train = df[df[\"review_date\"] <= cutoff].copy()\n",
    "    df_test  = df[df[\"review_date\"] >  cutoff].copy()\n",
    "\n",
    "    # ===== align feature cols =====\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "\n",
    "    # ===== user_profiles from TRAIN only =====\n",
    "    user_profiles = build_user_profiles(df_train, feature_cols)\n",
    "\n",
    "    # ===== seen from TRAIN only =====\n",
    "    seen = (\n",
    "        df_train.groupby(\"user_id\")[\"business_id\"]\n",
    "        .apply(set)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # ===== ground truth from TEST positives only =====\n",
    "    pos_test = df_test[df_test[\"label\"] == 1][[\"user_id\", \"business_id\"]].copy()\n",
    "    gt = pos_test.groupby(\"user_id\")[\"business_id\"].apply(set).to_dict()\n",
    "\n",
    "    # ===== item embedding tensor =====\n",
    "    item_emb_t = torch.from_numpy(item_emb).to(device)\n",
    "\n",
    "    recalls, ndcgs = [], []\n",
    "\n",
    "    users = list(user_profiles.index)\n",
    "    users = [u for u in gt.keys() if u in user_profiles.index]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for uid in users:\n",
    "            true_items = gt.get(uid, set()) - seen.get(uid, set())\n",
    "            if not true_items:\n",
    "                continue\n",
    "\n",
    "            # user vector input (same preprocess as training)\n",
    "            u = user_profiles.loc[uid, feature_cols].to_numpy(dtype=np.float32).reshape(1, -1)\n",
    "            u_df = pd.DataFrame(u, columns=feature_cols)\n",
    "            u_x = numeric_log1p_clip(u_df, feature_cols)\n",
    "            u_x = scaler.transform(u_x).astype(np.float32)\n",
    "            user_tensor = torch.from_numpy(u_x).to(device)\n",
    "\n",
    "            user_vec = model.user_tower(user_tensor)              # [1, d]\n",
    "            scores = torch.matmul(item_emb_t, user_vec.T).squeeze(1)  # [num_items]\n",
    "\n",
    "            # ===== mask seen BEFORE topk =====\n",
    "            seen_set = seen.get(uid, set())\n",
    "            if seen_set:\n",
    "                mask = np.isin(item_ids, np.array(list(seen_set), dtype=object))\n",
    "                if mask.any():\n",
    "                    scores = scores.masked_fill(torch.from_numpy(mask).to(device), float(\"-inf\"))\n",
    "\n",
    "            topk = torch.topk(scores, k=cfg.k)\n",
    "            rec = item_ids[topk.indices.detach().cpu().numpy()].tolist()\n",
    "\n",
    "            hits = [i for i, bid in enumerate(rec, start=1) if bid in true_items]\n",
    "            denom = min(len(true_items), cfg.k)\n",
    "            recall = (len(hits) / denom) if denom > 0 else 0.0\n",
    "\n",
    "            recalls.append(recall)\n",
    "            ndcgs.append(ndcg_at_k(hits, cfg.k))\n",
    "\n",
    "    print(f\"cutoff: {cutoff}\")\n",
    "    print(f\"Users evaluated: {len(recalls)}\")\n",
    "    print(f\"Recall@{cfg.k}: {float(np.mean(recalls)):.4f}\")\n",
    "    print(f\"NDCG@{cfg.k}:  {float(np.mean(ndcgs)):.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
